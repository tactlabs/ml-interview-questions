---
title: "15 things to avoid"
date: 2021-07-13T19:40:39+05:30
draft: false
---

**15 Data fallacies to Avoid**

The common refrain is: Lies, damned lies, and statistics.

Check out these common data fallacies beautifully illustrated and summarized by Geckoboard.

**1. Cherry Picking**

Select data that supports your claim, and hide that does not. Always ask: Is it full data? What filters have been applied?

**2. Data Dredging**

Hiding that correlation is not statistically significant. Instead of defining a hypothesis up front, it is switched after data dredging.

**3. Survivorship Bias**

When you draw a conclusion based on incomplete data, specifically only the sample that "survived" some selection criteria.

**4. Cobra Effect: Law of unintended consequences**

When an incentive produces the opposite result than intended. Also known as a Perverse Incentive.

**5. False Causality**

Famous "Correlation is not causation". To falsely assume when two events occur together that one must have caused the other.

**6. Gerrymandering**

Manipulating cohort definition to demonstrate desired results. For example, manipulating boundaries of political districts in order to sway the result of an election.

**7. Sampling Bias**

Drawing conclusions from a set of data that isn’t representative of the population you’re trying to understand.

**8. Gambler's Fallacy**

The mistaken belief that because something has happened more frequently than usual, it’s now less likely to happen in the future and vice versa.

**9. Regression Toward the Mean**

Everything, good or bad, over time it will revert back towards the average. Do not assume that predictions about best or worst picks of any kind will hold true forever.

**10. Hawthorne Effect**

When the act of monitoring someone can affect that person’s behavior. Also known as the Observer Effect. Useful to keep in mind when research subjects are human.

**11. Simpson's Paradox**

The trend in the combined data is the opposite of the trend seen on different groups of data.

**12. McNamara Fallacy**
A complex situation can't be solely analyzed through metrics. You may miss the bigger picture. Robert McNamara, the U.S. Secretary of Defense, took the approach of using enemy body count as a measure of Vietnam War success. That missed factors such as shifting public opinion.

**13. Overfitting**

A more complex explanation will often describe your data better than a simple one. However, a simpler explanation is usually more representative of the underlying relationship.

**14. Publication Bias**

How interesting a research finding is affects how likely it is to be published, distorting our impression of reality.

**15. Danger of Summary Metrics**

Francis Anscombe put together four example data sets in the 1970s. Known as Anscombe’s Quartet, each data set has the same mean, variance and correlation. However, when graphed, it’s clear that each of the data sets are totally different.